{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev-Nandan-Kumar/ML-use-cases/blob/main/Pyspark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jeBKIRBl6Xi",
        "outputId": "d95b5827-83b5-48d1-8772-e867d8e5ed6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=00108957fa54b4087d31e262c1b1cfcd147afd9001a2736ebc9c417c74aa96b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "0vWqVCDkxFai",
        "outputId": "295503e9-4cd9-48d8-9f70-b17719b94d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.0-preview2/spark-3.5.1-preview2-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyXv7VuRZE93",
        "outputId": "b380f4aa-1acd-4a21-8fd4-1bb3862aad32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-29 09:09:36--  https://downloads.apache.org/spark/spark-3.0.0-preview2/spark-3.5.1-preview2-bin-hadoop3.2.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-07-29 09:09:36 ERROR 404: Not Found.\n",
            "\n",
            "tar: spark-3.0.0-preview2-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhpnA5UIl0ug",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "ff4df3ee-c5ed-4cbf-bbff-d4ebc6177c6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3e51e97f758b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"Couldn't find Spark, make sure SPARK_HOME env is set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m\" or Spark is in an expected location (e.g. from homebrew installation).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GATkzNwOplYk",
        "outputId": "1b64c0ea-59e9-49f5-93ed-58f7cb1ff554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|  Spark|\n",
            "+-------+\n",
            "|Jupyter|\n",
            "|Jupyter|\n",
            "|Jupyter|\n",
            "|Jupyter|\n",
            "|Jupyter|\n",
            "+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Create Dataframe\n",
        "df1 = spark.createDataFrame([{\"Spark\": \"Jupyter\"} for x in range(1000)])\n",
        "df1.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUHSOUjzBPGG"
      },
      "outputs": [],
      "source": [
        "data = [{\"Category\": 'A', \"ID\": 1, \"Value\": 121.44, \"Truth\": True},\n",
        "        {\"Category\": 'B', \"ID\": 5, \"Value\": 300.01, \"Truth\": False},\n",
        "        {\"Category\": 'G', \"ID\": 6, \"Value\": 308.01, \"Truth\": False},\n",
        "        {\"Category\": 'B', \"ID\": 7, \"Value\": 30.01, \"Truth\": False},\n",
        "        {\"Category\": 'A', \"ID\": 8, \"Value\": 306.01, \"Truth\": False},\n",
        "        {\"Category\": 'B', \"ID\": 9, \"Value\": 300.01, \"Truth\": True},\n",
        "        {\"Category\": 'D', \"ID\": 10, \"Value\": 300.01, \"Truth\": False},\n",
        "        {\"Category\": 'B', \"ID\": 2, \"Value\": 300.01, \"Truth\": False},\n",
        "        {\"Category\": 'C', \"ID\": 3, \"Value\": 10.99, \"Truth\": None},\n",
        "        {\"Category\": 'E', \"ID\": 4, \"Value\": 33.87, \"Truth\": True},\n",
        "        {\"Category\": 'F', \"ID\": 4, \"Value\": 93.87, \"Truth\": True}\n",
        "\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AdhAoIBBQBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8ad012eb-dab5-45f4-9c98-5277486dabec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-51d7ede7641b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWYbEJxHBfll",
        "outputId": "457a10ee-8233-4eaf-8393-a18afb93c25c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.createDataFrame(data)\n",
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX3oNJLcpv1H",
        "outputId": "3f785e6a-535b-41d0-a9a0-2efc2418858b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       A|  1| true|121.44|\n",
            "|       B|  5|false|300.01|\n",
            "|       G|  6|false|308.01|\n",
            "|       B|  7|false| 30.01|\n",
            "|       A|  8|false|306.01|\n",
            "|       B|  9| true|300.01|\n",
            "|       D| 10|false|300.01|\n",
            "|       B|  2|false|300.01|\n",
            "|       C|  3| null| 10.99|\n",
            "|       E|  4| true| 33.87|\n",
            "|       F|  4| true| 93.87|\n",
            "+--------+---+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating Columns\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb48e5EkCPxu",
        "outputId": "3134f670-b5c6-4564-a0f3-2647d70292fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+------+\n",
            "|Category| ID|Truth| Value|newcol|\n",
            "+--------+---+-----+------+------+\n",
            "|       A|  1| true|121.44|     2|\n",
            "|       B|  5|false|300.01|     6|\n",
            "|       G|  6|false|308.01|     7|\n",
            "|       B|  7|false| 30.01|     8|\n",
            "|       A|  8|false|306.01|     9|\n",
            "|       B|  9| true|300.01|    10|\n",
            "|       D| 10|false|300.01|    11|\n",
            "|       B|  2|false|300.01|     3|\n",
            "|       C|  3| null| 10.99|     4|\n",
            "|       E|  4| true| 33.87|     5|\n",
            "|       F|  4| true| 93.87|     5|\n",
            "+--------+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=df.withColumn('newcol', df.ID+1)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4K6DMf3EaxP",
        "outputId": "cf39579c-55e6-40e9-87ca-904e0976cb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+------+\n",
            "|Category| ID|Truth| Value|newcol|\n",
            "+--------+---+-----+------+------+\n",
            "|       E|  4| true| 33.87|     5|\n",
            "|       F|  4| true| 93.87|     5|\n",
            "|       A|  1| true|121.44|     2|\n",
            "|       B|  9| true|300.01|    10|\n",
            "+--------+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.Truth==True).sort(df.Value).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmt6JyUSEvEi",
        "outputId": "9f18eea1-370e-4a13-a0a7-7ad75950278f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+------+\n",
            "|Category| ID|Truth| Value|newcol|\n",
            "+--------+---+-----+------+------+\n",
            "|       E|  4| true| 33.87|     5|\n",
            "|       F|  4| true| 93.87|     5|\n",
            "|       A|  1| true|121.44|     2|\n",
            "|       B|  9| true|300.01|    10|\n",
            "+--------+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.createOrReplaceTempView('table')\n",
        "spark.sql(''' SELECT * FROM table WHERE Truth==true ORDER BY Value ASC ''').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1FiVg6QGVWl",
        "outputId": "8b114922-23b7-4873-d70a-63c56f2a0437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Category: string (nullable = true)\n",
            " |-- ID: long (nullable = true)\n",
            " |-- Truth: boolean (nullable = true)\n",
            " |-- Value: double (nullable = true)\n",
            " |-- newcol: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESCKokfEHVQt"
      },
      "outputs": [],
      "source": [
        "### Select and filter\n",
        "df.select('ID', 'Truth').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MilzITAIHo5Z"
      },
      "outputs": [],
      "source": [
        "df.filter(\"ID>2\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asJBq7sLMufd"
      },
      "outputs": [],
      "source": [
        "df.filter(\"Truth like 't%'\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ1zvosRNcsz"
      },
      "outputs": [],
      "source": [
        "# Multiple filter chaining\n",
        "df.filter(\"Truth like 't%'\").filter(\"Category=='A'\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbZRREd4N5bw"
      },
      "outputs": [],
      "source": [
        "df.filter(\"ID in (1,2,4)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyMBHKMPONt7"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"Truth\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHVERkgPOlRa"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"Truth\").count().filter(\"Truth=='true'\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWzFOlZsO1yq"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"Truth\").count().orderBy(\"Truth\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlU8CddvRTsu"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKfNkT3xPXkt",
        "outputId": "3aa9eb3c-0b3d-4f4d-bf95-08d4e89c38f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Billing Date0: string (nullable = true)\n",
            " |-- Month: integer (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Plnt: string (nullable = true)\n",
            " |-- Sold-to pt: integer (nullable = true)\n",
            " |-- Material: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Sub Category: string (nullable = true)\n",
            " |-- Model: string (nullable = true)\n",
            " |-- Factory: string (nullable = true)\n",
            " |-- Capa.: string (nullable = true)\n",
            " |-- Manual Serial No: string (nullable = true)\n",
            " |-- Prod. Month: string (nullable = true)\n",
            " |-- Mfg. Period: string (nullable = true)\n",
            " |-- Sale Month: string (nullable = true)\n",
            " |-- Billing Date15: string (nullable = true)\n",
            "\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|    05-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|    10-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DataFrame Query: Cast columns to specific data type\n",
        "dfc = spark.read.format('csv').options(header='true', inferSchema='true').load(\"/content/Primary_Sales_Till_March.csv\")\n",
        "\n",
        "dfc.printSchema()\n",
        "dfc.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u44aNg4qp6z8",
        "outputId": "2f6cc4d2-9d54-459c-f65a-62b93c9f35c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(79022, 16)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfc.count(), len(dfc.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VmeioZmuJSL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u21Au3EEuS7D",
        "outputId": "4db68a4d-bf40-4b38-d451-b830e224c155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|min(Year)|\n",
            "+---------+\n",
            "|     2019|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.agg(F.min('Year')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsPAeZQ-uzAb",
        "outputId": "f192fe20-a21f-4d5f-dca1-b605168889d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2019"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfc.agg(F.min('Year')).collect()[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrdwynReu-2I",
        "outputId": "f97e996e-e5a5-4a3a-bd10-53dfa2ded9ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(Billing Date0='05.01.2019', Month=1, Year=2019, Plnt='V255', Sold-to pt=160070, Material=1200018, Product='Washing Machine', Sub Category='FATL', Model='WTL62S', Factory='Beko/Defy', Capa.='Upto 6.5 Kg', Manual Serial No='1200018A18F900012', Prod. Month='2018F', Mfg. Period=\"Sep'18-Mar'19\", Sale Month='Jan-19', Billing Date15='05-01-2019'),\n",
              " Row(Billing Date0='07.01.2019', Month=1, Year=2019, Plnt='V255', Sold-to pt=26114942, Material=1200018, Product='Washing Machine', Sub Category='FATL', Model='WTL62S', Factory='Beko/Defy', Capa.='Upto 6.5 Kg', Manual Serial No='1200018A18F001517', Prod. Month='2018F', Mfg. Period=\"Sep'18-Mar'19\", Sale Month='Jan-19', Billing Date15='07-01-2019'),\n",
              " Row(Billing Date0='07.01.2019', Month=1, Year=2019, Plnt='V255', Sold-to pt=26114942, Material=1200018, Product='Washing Machine', Sub Category='FATL', Model='WTL62S', Factory='Beko/Defy', Capa.='Upto 6.5 Kg', Manual Serial No='1200018A18F001502', Prod. Month='2018F', Mfg. Period=\"Sep'18-Mar'19\", Sale Month='Jan-19', Billing Date15='07-01-2019'),\n",
              " Row(Billing Date0='07.01.2019', Month=1, Year=2019, Plnt='V255', Sold-to pt=26114942, Material=1200007, Product='Washing Machine', Sub Category='FAFL', Model='WFL60WS', Factory='Arcelik', Capa.='Upto 6.5 Kg', Manual Serial No='1200007A18F001016', Prod. Month='2018F', Mfg. Period=\"Sep'18-Mar'19\", Sale Month='Jan-19', Billing Date15='07-01-2019'),\n",
              " Row(Billing Date0='07.01.2019', Month=1, Year=2019, Plnt='V255', Sold-to pt=26114942, Material=1200006, Product='Washing Machine', Sub Category='FAFL', Model='WFL60SS', Factory='Arcelik', Capa.='Upto 6.5 Kg', Manual Serial No='1200006A18D000723', Prod. Month='2018D', Mfg. Period=\"Sep'18-Mar'19\", Sale Month='Jan-19', Billing Date15='07-01-2019')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfc.collect()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49rHp-idvNYs",
        "outputId": "bd43a1de-5b72-406b-8a8e-a6b29b94d1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|        Product|\n",
            "+---------------+\n",
            "|   Refrigerator|\n",
            "| Microwave Oven|\n",
            "|Washing Machine|\n",
            "|     Dishwasher|\n",
            "|          Dryer|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Transformations\n",
        "dfc.select('Product').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLYhEdFvd7Q",
        "outputId": "16da7cfc-c45c-4b7a-a5f7-77e7085e9179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-----------------------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|Billing Date15|Time Series Forecasting|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-----------------------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|    05-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|                    Yes|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|                    Yes|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|    09-01-2019|                    Yes|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|                    Yes|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|                    Yes|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|    10-01-2019|                    Yes|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|                    Yes|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|                    Yes|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|    11-01-2019|                    Yes|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# withColumn: Create new columns & withColumnRenamed\n",
        "updated_dfc=dfc.withColumn(\"Time Series Forecasting\", F.lit(\"Yes\"))\n",
        "updated_dfc.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "wJAniotpxwee",
        "outputId": "19e02ea5-6e16-483a-a9d3-f95c4a67ece7"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-8a3c42a757fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    up_dfc=dfc.withColumn(\"label\", F.when(F.col(\"Product\")==\"Washing Machine\", 0)                      .when(F.col(\"Product\")==\"Refrigerator\", 1)                      .when(F.col(\"Product\")==\"Microwave Oven\", 2)                      .when(F.col(\"Product\")==\"Dishwasher\", 3)                      .when(\"Product\")==\"Dryer\", 4)                      .otherwise(5))\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "up_dfc=dfc.withColumn(\"label\", F.when(F.col(\"Product\")==\"Washing Machine\", 0)\\\n",
        "                      .when(F.col(\"Product\")==\"Refrigerator\", 1)\\\n",
        "                      .when(F.col(\"Product\")==\"Microwave Oven\", 2)\\\n",
        "                      .when(F.col(\"Product\")==\"Dishwasher\", 3)\\\n",
        "                      .when(\"Product\")==\"Dryer\", 4)\\\n",
        "                      .otherwise(5))\n",
        "up_dfc.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn0YrDzn5aRK",
        "outputId": "764c1fa7-881e-4bfc-d712-aaa2141e7d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+----------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|        BD|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+----------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|05-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|07-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|09-01-2019|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|10-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|11-01-2019|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dff=dfc.withColumnRenamed(\"Billing Date15\", \"BD\")\n",
        "dff.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6PWYuYd6eSp",
        "outputId": "14c2694d-369d-42d1-fbf0-1c46b8846cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----------+----------+\n",
            "|        Product|   min(BD)|   max(BD)|\n",
            "+---------------+----------+----------+\n",
            "|     Dishwasher|02-05-2019|31-12-2019|\n",
            "|          Dryer|03-12-2019|31-12-2019|\n",
            "| Microwave Oven|01-02-2019|31-10-2019|\n",
            "|   Refrigerator|01-02-2019|31-12-2019|\n",
            "|Washing Machine|01-02-2019|31-12-2019|\n",
            "+---------------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dff.groupby(\"Product\").agg(F.min(\"BD\"), F.max(\"BD\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ40kS-y62sv"
      },
      "outputs": [],
      "source": [
        "## User defined Functions\n",
        "## STEP 1: Create a python function\n",
        "\n",
        "def transform_income(income_str):\n",
        "\n",
        "    income_str = str(income_str)\n",
        "\n",
        "    if income_str[0] == \"L\":\n",
        "        income_str = income_str.split(\" \")[-1]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    elif income_str[-1] == \"e\":\n",
        "        income_str = income_str.split(\" \")[0]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    else:\n",
        "        income_str = income_str.split(\" - \")\n",
        "        avg_income = (int(income_str[0][1:]) + int(income_str[1][1:]))/2\n",
        "        return avg_income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOG1GeYcB4wv",
        "outputId": "4c3018bc-2fcc-42b8-80f4-767f2739fc9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12500.0, 100000.0, 43749.5)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_income(\"Less than $12500\"), transform_income(\"$100000 or More\"), transform_income(\"$37500 - $49999\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxCMPLRVu65a"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import types as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjgz4JTBCJBl"
      },
      "outputs": [],
      "source": [
        "## STEP 2: Convert python function to UDF function\n",
        "transform_income_udf = F.udf(f=lambda row: transform_income(row), returnType=T.FloatType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYMiB0szCaub"
      },
      "outputs": [],
      "source": [
        "updated_spark_df = dfc.withColumn(\"Material_test\", transform_income_udf(F.col(\"Year\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyQo4IGmCa5I",
        "outputId": "16b074c6-1a43-4971-ed8c-cce490cb2f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "|summary|Billing Date0|             Month|              Year| Plnt|        Sold-to pt|          Material|        Product|  Sub Category| Model|Factory|    Capa.| Manual Serial No|        Prod. Month|  Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "|  count|        79022|             79022|             79022|79022|             79022|             79022|          79022|         79022| 79022|  75641|    79022|            79022|              79022|        79022|     79022|         79022|\n",
            "|   mean|         null| 6.107653564830048|2019.1330642099667| null|1607439.9285641974|1164175.0780289033|           null|          null|  null|   null|     null|             null| 2018.3853110106318|         null|      null|          null|\n",
            "| stddev|         null|3.6552660308716796|0.3396462657447687| null| 6087613.727848912|  58779.1236778935|           null|          null|  null|   null|     null|             null|0.48743633500988937|         null|      null|          null|\n",
            "|    min|   01.02.2019|                 1|              2019| V255|            100038|           1100001|     Dishwasher|Bottom Mounted| DF14S|Arcelik|300 - 500|1100001A18F000003|              2015F|Apr'19-Sep'19|    Apr-19|    01-02-2019|\n",
            "|    max|   31.12.2019|                12|              2020| V482|          28012965|           1500001|Washing Machine|     Table Top|WWD80S|Voltbek|Upto 7 Kg|1500001A18F000381|              2020B|Sep'18-Mar'19|    Sep-19|    31-12-2019|\n",
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4cEp0uEEG7v",
        "outputId": "4efa22d5-7e5b-47fb-bad3-c31c0a4250a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "|summary|Billing Date0|             Month|              Year| Plnt|        Sold-to pt|          Material|        Product|  Sub Category| Model|Factory|    Capa.| Manual Serial No|        Prod. Month|  Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "|  count|        79022|             79022|             79022|79022|             79022|             79022|          79022|         79022| 79022|  75641|    79022|            79022|              79022|        79022|     79022|         79022|\n",
            "|   mean|         null| 6.107653564830048|2019.1330642099667| null|1607439.9285641974|1164175.0780289033|           null|          null|  null|   null|     null|             null| 2018.3853110106318|         null|      null|          null|\n",
            "| stddev|         null|3.6552660308716796|0.3396462657447687| null| 6087613.727848912|  58779.1236778935|           null|          null|  null|   null|     null|             null|0.48743633500988937|         null|      null|          null|\n",
            "|    min|   01.02.2019|                 1|              2019| V255|            100038|           1100001|     Dishwasher|Bottom Mounted| DF14S|Arcelik|300 - 500|1100001A18F000003|              2015F|Apr'19-Sep'19|    Apr-19|    01-02-2019|\n",
            "|    25%|         null|                 2|              2019| null|            101958|           1100028|           null|          null|  null|   null|     null|             null|             2018.0|         null|      null|          null|\n",
            "|    50%|         null|                 6|              2019| null|            102690|           1200019|           null|          null|  null|   null|     null|             null|             2018.0|         null|      null|          null|\n",
            "|    75%|         null|                 9|              2019| null|            121876|           1200064|           null|          null|  null|   null|     null|             null|             2019.0|         null|      null|          null|\n",
            "|    max|   31.12.2019|                12|              2020| V482|          28012965|           1500001|Washing Machine|     Table Top|WWD80S|Voltbek|Upto 7 Kg|1500001A18F000381|              2020B|Sep'18-Mar'19|    Sep-19|    31-12-2019|\n",
            "+-------+-------------+------------------+------------------+-----+------------------+------------------+---------------+--------------+------+-------+---------+-----------------+-------------------+-------------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.summary().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bia0YGV6EJ5H",
        "outputId": "ddfc91c7-ffcb-4502-f1d0-87549885bfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------------+-----+----+----+----------+--------+-------+------------+-----+-------+-----+----------------+-----------+-----------+----------+--------------+\n",
            "|summary|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|Product|Sub Category|Model|Factory|Capa.|Manual Serial No|Prod. Month|Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------+-------------+-----+----+----+----------+--------+-------+------------+-----+-------+-----+----------------+-----------+-----------+----------+--------------+\n",
            "|    40%|         null|    5|2019|null|    102597| 1100133|   null|        null| null|   null| null|            null|     2018.0|       null|      null|          null|\n",
            "|    60%|         null|    7|2019|null|    102774| 1200028|   null|        null| null|   null| null|            null|     2018.0|       null|      null|          null|\n",
            "|    90%|         null|   11|2020|null|    125111| 1200115|   null|        null| null|   null| null|            null|     2019.0|       null|      null|          null|\n",
            "+-------+-------------+-----+----+----+----------+--------+-------+------------+-----+-------+-----+----------------+-----------+-----------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.summary(\"40%\", \"60%\", \"90%\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKDMUHsFJLXF",
        "outputId": "037b68f5-d958-4400-a21b-d4f9f7df5980"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------+-------------+\n",
            "| id|category|categoryIndex|\n",
            "+---+--------+-------------+\n",
            "|  0|       a|          0.0|\n",
            "|  1|       b|          2.0|\n",
            "|  2|       c|          1.0|\n",
            "|  3|       a|          0.0|\n",
            "|  4|       a|          0.0|\n",
            "|  5|       c|          1.0|\n",
            "+---+--------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Spark Label encoder\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(spark)\n",
        "\n",
        "df = sqlContext.createDataFrame(\n",
        "            [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
        "            [\"id\", \"category\"])\n",
        "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
        "indexed = indexer.fit(df).transform(df)\n",
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDu92dCoNv7G",
        "outputId": "d92dedf0-6036-4643-f7a7-ab53f43ccd66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-------------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|Billing Date15|categoryIndex|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-------------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|    05-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          1.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|          0.0|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|          0.0|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|    09-01-2019|          0.0|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|          0.0|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|          0.0|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|    10-01-2019|          0.0|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|          0.0|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|          0.0|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|    11-01-2019|          0.0|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexer1 = StringIndexer(inputCol=\"Product\", outputCol=\"categoryIndex\")\n",
        "indexed1 = indexer1.fit(dfc).transform(dfc)\n",
        "indexed1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUiuFZKPOXrY",
        "outputId": "a7c6cb81-f775-472d-c412-c030c90d4af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|CategoryIndex|\n",
            "+-------------+\n",
            "|          0.0|\n",
            "|          1.0|\n",
            "|          4.0|\n",
            "|          3.0|\n",
            "|          2.0|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed1.select(\"CategoryIndex\").distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MaYP4ISUEAx",
        "outputId": "bbc58ce4-ad93-40c7-d3c2-9430b044a69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|        Product|\n",
            "+---------------+\n",
            "|   Refrigerator|\n",
            "| Microwave Oven|\n",
            "|Washing Machine|\n",
            "|     Dishwasher|\n",
            "|          Dryer|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(\"Product\").distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M86L_5IWkUC",
        "outputId": "51c72bf7-e9a2-4091-b302-57b3de1f63f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79022"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfc.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXt0at_yEkRc",
        "outputId": "66b2216b-5a36-47bb-ec75-c499c79b0f81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(108623, 5159)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Join\n",
        "dfwr=dfc.filter((F.col(\"Product\")== 'Washing Machine') | (F.col(\"Product\")=='Refrigerator'))\n",
        "dfdm=dfc.filter((F.col(\"Product\")== 'Microwave Oven') | (F.col(\"Product\")=='Dryer'))\n",
        "\n",
        "dfwr.count(), dfdm.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfQf1OxIVDJw",
        "outputId": "c2c65b93-b4d9-4df8-d6be-2d85ee212ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "122631"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfwr.join(dfc, \"Manual Serial No\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywlFLNJo1RSd",
        "outputId": "32bcad97-d550-4933-b629-b213ea62849a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       A|  1| true|121.44|\n",
            "|       B|  2|false|300.01|\n",
            "|       C|  3| null| 10.99|\n",
            "|       E|  4| true| 33.87|\n",
            "|       F|  4| true| 93.87|\n",
            "+--------+---+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftfoAJi21Kw2",
        "outputId": "7c45f1d6-1b79-4160-c7af-bbfa1f329098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       A|  1| true|121.44|\n",
            "|       E|  4| true| 33.87|\n",
            "|       F|  4| true| 93.87|\n",
            "+--------+---+-----+------+\n",
            "\n",
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       B|  2|false|300.01|\n",
            "|       C|  3| null| 10.99|\n",
            "+--------+---+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1=df.filter((F.col(\"ID\")== 1) | (F.col(\"ID\")==4))\n",
        "df2=df.filter((F.col(\"ID\")== 2) | (F.col(\"ID\")==3))\n",
        "df1.count(), df2.count()\n",
        "df1.show()\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "KPNfGmAjUtBR",
        "outputId": "6f3c02ea-0c5a-4100-eddc-e7ebcdc3c36f"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-4d904139a7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# join on explicit columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
          ]
        }
      ],
      "source": [
        "# join on explicit columns\n",
        "df1.join(df2, df2(\"ID\")==df1(\"ID\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1saNwMDTrc9",
        "outputId": "6c795d15-b3a9-4893-b7d6-c08bd57e515f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+------+\n",
            "|Category| ID|Truth| Value|newcol|\n",
            "+--------+---+-----+------+------+\n",
            "|       A|  1| true|121.44|     2|\n",
            "|       B|  5|false|300.01|     6|\n",
            "|       G|  6|false|308.01|     7|\n",
            "|       B|  7|false| 30.01|     8|\n",
            "|       A|  8|false|306.01|     9|\n",
            "|       B|  9| true|300.01|    10|\n",
            "|       D| 10|false|300.01|    11|\n",
            "|       B|  2|false|300.01|     3|\n",
            "|       C|  3| null| 10.99|     4|\n",
            "|       E|  4| true| 33.87|     5|\n",
            "|       F|  4| true| 93.87|     5|\n",
            "+--------+---+-----+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxQcEjCtgCq9"
      },
      "outputs": [],
      "source": [
        "approx_count_distinct\n",
        "avg\n",
        "collect_list\n",
        "collect_set\n",
        "countDistinct\n",
        "count\n",
        "grouping\n",
        "first\n",
        "last\n",
        "kurtosis\n",
        "max\n",
        "min\n",
        "mean\n",
        "skewness\n",
        "stddev\n",
        "stddev_samp\n",
        "stddev_pop\n",
        "sum\n",
        "sumDistinct\n",
        "variance, var_samp, var_pop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EriRm3GThnv2",
        "outputId": "3a512eb5-bc48-42e8-c369-61879b525362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "df_s = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df_s.printSchema()\n",
        "df_s.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfistlMcMUi",
        "outputId": "e9f13959-11c9-410c-cd80-32ed783924b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       A|  1| true|121.44|\n",
            "|       B|  5|false|300.01|\n",
            "|       G|  6|false|308.01|\n",
            "|       B|  7|false| 30.01|\n",
            "|       A|  8|false|306.01|\n",
            "|       B|  9| true|300.01|\n",
            "|       D| 10|false|300.01|\n",
            "|       B|  2|false|300.01|\n",
            "|       C|  3| null| 10.99|\n",
            "|       E|  4| true| 33.87|\n",
            "|       F|  4| true| 93.87|\n",
            "+--------+---+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pGG-gCNatnL",
        "outputId": "78d5a0e6-2bca-4eca-bec1-9a3e42f24c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(Category='A', ID=1, Truth=True, Value=121.44), Row(Category='B', ID=5, Truth=False, Value=300.01), Row(Category='G', ID=6, Truth=False, Value=308.01)]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "## sampleBy for stratified data\n",
        "print(df.sample(0.2).collect())\n",
        "print(df.sampleBy(\"Value\",{0:0.5, 1:0.5}, 0).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyVIKeGciq37",
        "outputId": "2418d818-46ab-44d0-de80-6f2b449a08dc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-54764f8648aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_s' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"avg: \" + str(df_s.select(avg(\"salary\")).collect()[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecRirjMlvB1w"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, approx_count_distinct, avg,collect_list,collect_set, countDistinct,count,grouping,first,last,kurtosis,max,min,mean,skewness,stddev,stddev_samp,stddev_pop,sum,sumDistinct,variance, var_samp, var_pop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A3HdD5BgFXJ"
      },
      "outputs": [],
      "source": [
        "#//approx_count_distinct()\n",
        "\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df_s.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msPDkr-1r8K8",
        "outputId": "168c817e-8445-4ca5-d095-5ebb04e97ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+\n",
            "|collect_list(Product)|\n",
            "+---------------------+\n",
            "| [Washing Machine,...|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(collect_list(\"Product\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25QEktp-tIY2",
        "outputId": "a13c69aa-94f2-4294-af94-2f5c8f5016fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|collect_set(Product)|\n",
            "+--------------------+\n",
            "|[Dryer, Washing M...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(collect_set(\"Product\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5gmwnwktUVQ",
        "outputId": "077ddcfb-48a2-4b2c-f9eb-84dccbf694dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------+\n",
            "|count(DISTINCT Product, plnt)|\n",
            "+-----------------------------+\n",
            "|                           29|\n",
            "+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfg=dfc.select(countDistinct(\"Product\", \"plnt\"))\n",
        "dfg.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9alQ9phtvWf",
        "outputId": "3f2a9284-1d71-460d-f1b9-d4bace7ccbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product Count28681\n"
          ]
        }
      ],
      "source": [
        "print(\"Product Count\"+str(dfc.select(count(\"Product\")).collect()[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeysMm69uFg2",
        "outputId": "fb4ba745-2c08-4488-c5e6-c1d70f17a773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|first(plnt)|\n",
            "+-----------+\n",
            "|       V255|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(first(\"plnt\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crCevCUEuhyE",
        "outputId": "4723fb9a-1019-411c-8bbb-38d5dfb8d4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|last(Billing Date15)|\n",
            "+--------------------+\n",
            "|          29-08-2019|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(last(\"Billing Date15\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMD9Gqq2vEd1",
        "outputId": "0681b10b-3515-42b5-fc01-c5ab0666ebb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_s.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUdBR8dou3Vv",
        "outputId": "f615ca70-6493-4355-a2b2-8a7990cb54f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|   kurtosis(salary)|\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_s.select(kurtosis(\"salary\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXEmNlyUvSoF",
        "outputId": "16f92aaa-cf57-4cc0-ffe1-43dffe8f7d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|max(Billing Date15)|\n",
            "+-------------------+\n",
            "|         31-12-2019|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(max(\"Billing Date15\")).show() # min, mean, sum, sumDistinct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuZ4TFJqvhsv",
        "outputId": "527a0d1f-03b8-4f71-ab87-a6640617fbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|    skewness(salary)|\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_s.select(skewness(\"salary\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohSC0RhKvrLl",
        "outputId": "56438ef8-d8a6-449e-fcd2-8dd2fef33d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|stddev_samp(salary)|\n",
            "+-------------------+\n",
            "|  765.9416862050705|\n",
            "+-------------------+\n",
            "\n",
            "+-------------------+\n",
            "|stddev_samp(salary)|\n",
            "+-------------------+\n",
            "|  765.9416862050705|\n",
            "+-------------------+\n",
            "\n",
            "+------------------+\n",
            "|stddev_pop(salary)|\n",
            "+------------------+\n",
            "|  726.636084983398|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_s.select(stddev(\"salary\")).show()\n",
        "df_s.select(stddev_samp(\"salary\")).show()\n",
        "df_s.select(stddev_pop(\"salary\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sEA_y0Xwl_I",
        "outputId": "8e5bc88a-2dd1-4476-993f-cd2cf8575ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "| var_samp(salary)|\n",
            "+-----------------+\n",
            "|586666.6666666666|\n",
            "+-----------------+\n",
            "\n",
            "+-----------------+\n",
            "| var_samp(salary)|\n",
            "+-----------------+\n",
            "|586666.6666666666|\n",
            "+-----------------+\n",
            "\n",
            "+---------------+\n",
            "|var_pop(salary)|\n",
            "+---------------+\n",
            "|       528000.0|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_s.select(variance(\"salary\")).show()\n",
        "df_s.select(var_samp(\"salary\")).show()\n",
        "df_s.select(var_pop(\"salary\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uNLMa7jwRIH"
      },
      "outputs": [],
      "source": [
        "## Window function\n",
        "ranking functions\n",
        "analytic functions\n",
        "aggregate functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijb9fw65yrqh",
        "outputId": "74829be6-30e1-4d99-d812-50ca1458c664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "dfw= spark.createDataFrame(data = simpleData, schema = columns)\n",
        "dfw.printSchema()\n",
        "dfw.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "8-FRXNkiy8Ij",
        "outputId": "ac20b1a0-6b3e-4ee1-ee03-110ee3729a49"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e2f5e4568fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"department\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row_number\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dfw' is not defined"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "win=Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "dfw.withColumn(\"row_number\", row_number().over(win)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ek2P7x89ZO",
        "outputId": "e23a70ab-a15e-48d4-9082-1c5a009c835e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|   1|\n",
            "|        Scott|   Finance|  3300|   2|\n",
            "|          Jen|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|         Jeff| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|       Robert|     Sales|  4100|   3|\n",
            "|         Saif|     Sales|  4100|   3|\n",
            "|      Michael|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|       Robert|     Sales|  4100|         3|\n",
            "|         Saif|     Sales|  4100|         3|\n",
            "|      Michael|     Sales|  4600|         5|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "+-------------+----------+------+-------+\n",
            "|employee_name|department|salary|Partion|\n",
            "+-------------+----------+------+-------+\n",
            "|        Maria|   Finance|  3000|    0.0|\n",
            "|        Scott|   Finance|  3300|    0.5|\n",
            "|          Jen|   Finance|  3900|    1.0|\n",
            "|        Kumar| Marketing|  2000|    0.0|\n",
            "|         Jeff| Marketing|  3000|    1.0|\n",
            "|        James|     Sales|  3000|    0.0|\n",
            "|        James|     Sales|  3000|    0.0|\n",
            "|       Robert|     Sales|  4100|    0.5|\n",
            "|         Saif|     Sales|  4100|    0.5|\n",
            "|      Michael|     Sales|  4600|    1.0|\n",
            "+-------------+----------+------+-------+\n",
            "\n",
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|        Maria|   Finance|  3000|    1|\n",
            "|        Scott|   Finance|  3300|    2|\n",
            "|          Jen|   Finance|  3900|    3|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|         Jeff| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|        James|     Sales|  3000|    2|\n",
            "|       Robert|     Sales|  4100|    3|\n",
            "|         Saif|     Sales|  4100|    4|\n",
            "|      Michael|     Sales|  4600|    5|\n",
            "+-------------+----------+------+-----+\n",
            "\n",
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|               Cum|\n",
            "+-------------+----------+------+------------------+\n",
            "|        Maria|   Finance|  3000|0.3333333333333333|\n",
            "|        Scott|   Finance|  3300|0.6666666666666666|\n",
            "|          Jen|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|         Jeff| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|       Robert|     Sales|  4100|               0.8|\n",
            "|         Saif|     Sales|  4100|               0.8|\n",
            "|      Michael|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary| lag|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|null|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|3000|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|       Robert|     Sales|  4100|3000|\n",
            "|         Saif|     Sales|  4100|3000|\n",
            "|      Michael|     Sales|  4600|4100|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|3900|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|null|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|       Robert|     Sales|  4100|4600|\n",
            "|         Saif|     Sales|  4100|null|\n",
            "|      Michael|     Sales|  4600|null|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import rank, percent_rank, ntile, cume_dist, lag, lead\n",
        "dfw.withColumn(\"rank\", (rank().over(win))).show()\n",
        "dfw.withColumn('dense rank', (rank().over(win))).show()\n",
        "dfw.withColumn('Partion', (percent_rank().over(win))).show()\n",
        "dfw.withColumn('ntile', (ntile(5).over(win))).show()\n",
        "dfw.withColumn(\"Cum\", (cume_dist().over(win))).show()\n",
        "dfw.withColumn(\"lag\", lag(\"salary\", 2).over(win)).show()\n",
        "dfw.withColumn(\"lead\", lead(\"salary\", 2).over(win)).show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T4hx9hS_xns"
      },
      "outputs": [],
      "source": [
        " ### Date and Timestamp\n",
        "Date Functions\n",
        "Timestamp Functions\n",
        "Date and Timestamp Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJUIaw4FBR20",
        "outputId": "77876972-680d-41b6-e9b7-91600e5a444a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|    05-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|    10-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AfowtSRFVQT",
        "outputId": "344459b2-388d-43d6-b462-900589b22ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "| id|     input|\n",
            "+---+----------+\n",
            "|  1|2020-02-01|\n",
            "|  2|2019-03-01|\n",
            "|  3|2021-03-01|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
        "dfd=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "dfd.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ILTAPL_Ebg",
        "outputId": "4678a131-47ea-4289-b9be-460a0256b88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|current_date()|\n",
            "+--------------+\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "|    2022-01-27|\n",
            "+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(current_date()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfoWydW0JQi_",
        "outputId": "76999c8e-0283-41d1-adb1-d12557fe084b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+\n",
            "|     input|Date_Format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 02-01-2020|\n",
            "|2019-03-01| 03-01-2019|\n",
            "|2021-03-01| 03-01-2021|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|     input|Date_Format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 2020-02-01|\n",
            "|2019-03-01| 2019-03-01|\n",
            "|2021-03-01| 2021-03-01|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|     input|Date_Format|\n",
            "+----------+-----------+\n",
            "|2020-02-01|        726|\n",
            "|2019-03-01|       1063|\n",
            "|2021-03-01|        332|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|     input|  Month_b/w|\n",
            "+----------+-----------+\n",
            "|2020-02-01|23.83870968|\n",
            "|2019-03-01|34.83870968|\n",
            "|2021-03-01|10.83870968|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfd.select(col(\"input\"), date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"Date_Format\")).show()\n",
        "dfd.select(col(\"input\"), to_date(col(\"input\"), \"yyyy-MM-dd\").alias(\"Date_Format\")).show()\n",
        "dfd.select(col(\"input\"), datediff(current_date(), col(\"input\")).alias(\"Date_Format\")).show()\n",
        "dfd.select(col(\"input\"), months_between(current_date(), col(\"input\")).alias(\"Month_b/w\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWraoW_4Kmej",
        "outputId": "4253b210-3f77-444d-ac86-31aa406f727c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+----------+----------+\n",
            "|     input|Month_trunc|Month_Year|Month_Year|\n",
            "+----------+-----------+----------+----------+\n",
            "|2020-02-01| 2020-02-01|2020-01-01|2020-02-01|\n",
            "|2019-03-01| 2019-03-01|2019-01-01|2019-03-01|\n",
            "|2021-03-01| 2021-03-01|2021-01-01|2021-03-01|\n",
            "+----------+-----------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfd.select(col(\"input\"),\n",
        "           trunc(col(\"input\"), \"Month\").alias(\"Month_trunc\"),\n",
        "           trunc(col(\"input\"), \"Year\").alias(\"Month_Year\"),\n",
        "           trunc(col(\"input\"), \"Month\").alias(\"Month_Year\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3bsLG48PM5x",
        "outputId": "c7f41bb7-303a-425a-ea1d-6d0515a4c0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+----------+----------+\n",
            "|     input|   added_m|     sub_m|      date|  date_sub|\n",
            "+----------+----------+----------+----------+----------+\n",
            "|2020-02-01|2020-06-01|2019-12-01|2020-02-06|2020-02-04|\n",
            "|2019-03-01|2019-07-01|2019-01-01|2019-03-06|2019-03-04|\n",
            "|2021-03-01|2021-07-01|2021-01-01|2021-03-06|2021-03-04|\n",
            "+----------+----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# add_months() , date_add(), date_sub()\n",
        "dfd.select(col(\"input\"),\n",
        "           add_months(col(\"input\"), 4).alias(\"added_m\"),\n",
        "           add_months(col(\"input\"), -2).alias(\"sub_m\"),\n",
        "           date_add(col(\"input\"), 5).alias(\"date\"),\n",
        "           date_sub(col(\"input\"), -3).alias(\"date_sub\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFuNi2rRAMy",
        "outputId": "a75cfb77-8a43-4bfc-d164-90d2b2b58fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+-----+----------+------------+-----------+------------+----------------+\n",
            "|     input|Year|Month| Next_date|Week_Of_Year|Day_of_week|Day_of_Month|dayofyear(input)|\n",
            "+----------+----+-----+----------+------------+-----------+------------+----------------+\n",
            "|2020-02-01|2020|    2|2020-02-02|           5|          7|           1|              32|\n",
            "|2019-03-01|2019|    3|2019-03-03|           9|          6|           1|              60|\n",
            "|2021-03-01|2021|    3|2021-03-07|           9|          2|           1|              60|\n",
            "+----------+----+-----+----------+------------+-----------+------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#year(), month(), month(),next_day(), weekofyear() dayofweek(), dayofmonth(), dayofyear()\n",
        "\n",
        "dfd.select(col(\"input\"),\n",
        "           year(col(\"input\")).alias(\"Year\"),\n",
        "           month(col(\"input\")).alias(\"Month\"),\n",
        "           next_day(col(\"input\"), \"Sunday\").alias(\"Next_date\"),\n",
        "           weekofyear(col(\"input\")).alias(\"Week_Of_Year\"),\n",
        "           dayofweek(col(\"input\")).alias(\"Day_of_week\"),\n",
        "           dayofmonth(col(\"input\")).alias(\"Day_of_Month\"),\n",
        "           dayofyear(\"input\")).alias(\"Day_of_Year\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da2QOZNTSA4z",
        "outputId": "0278004d-e589-4589-d185-09b759798b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|   current_timestamp|\n",
            "+--------------------+\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "|2022-01-27 06:57:...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfhms=dfc.select(current_timestamp().alias(\"current_timestamp\"))\n",
        "dfhms.select(col(\"current_timestamp\"),\n",
        "           year(col(\"\")).alias(\"Year\"),\n",
        "           year(col(\"input\")).alias(\"Year\"),\n",
        "           year(col(\"input\")).alias(\"Year\"),\n",
        "           year(col(\"input\")).alias(\"Year\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYBAhJupTYAR",
        "outputId": "c6ee38c3-a916-4b46-b205-ff586190bc3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "|     input|New_timestamp|\n",
            "+----------+-------------+\n",
            "|2020-02-01|         null|\n",
            "|2019-03-01|         null|\n",
            "|2021-03-01|         null|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.select(col(\"Billing Date15\"), to_timestamp(col(\"Billing Date15\"), \"MM-dd-yyyy HH mm ss SS\").alias(\"New_timestamp\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEq2L2R7Uyl5",
        "outputId": "aa10bb79-bd61-45f2-b56d-1eac499dba1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|Billing Date0|Month|Year|Plnt|Sold-to pt|Material|        Product|Sub Category|   Model|  Factory|          Capa.| Manual Serial No|Prod. Month|  Mfg. Period|Sale Month|Billing Date15|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "|   05.01.2019|    1|2019|V255|    160070| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F900012|      2018F|Sep'18-Mar'19|    Jan-19|    05-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001517|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001502|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001016|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18D000723|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200006|Washing Machine|        FAFL| WFL60SS|  Arcelik|    Upto 6.5 Kg|1200006A18C000686|      2018C|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114673| 1100017|   Refrigerator|  Frost Free|RFF383IF| Thailand|      300 - 500|1100017A18F000037|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001520|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001518|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26114942| 1200011|Washing Machine|        FAFL|  WFL70S|  Arcelik|> 6.5 <= 7.5 kg|1200011A18D000239|      2018D|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001547|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   07.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F001561|      2018F|Sep'18-Mar'19|    Jan-19|    07-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18F001031|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200007|Washing Machine|        FAFL| WFL60WS|  Arcelik|    Upto 6.5 Kg|1200007A18G001086|      2018G|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003397|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   09.01.2019|    1|2019|V255|  26117450| 1200018|Washing Machine|        FATL|  WTL62S|Beko/Defy|    Upto 6.5 Kg|1200018A18F003396|      2018F|Sep'18-Mar'19|    Jan-19|    09-01-2019|\n",
            "|   10.01.2019|    1|2019|V255|  26114942| 1200023|Washing Machine|        FATL|  WTL70S|Beko/Defy|> 6.5 <= 7.5 kg|1200023A18E000158|      2018E|Sep'18-Mar'19|    Jan-19|    10-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000802|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200016|Washing Machine|        FATL|  WTL60S|Beko/Defy|    Upto 6.5 Kg|1200016A18E000516|      2018E|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "|   11.01.2019|    1|2019|V255|    102689| 1200017|Washing Machine|        FATL|  WTL60G|Beko/Defy|    Upto 6.5 Kg|1200017A18F000516|      2018F|Sep'18-Mar'19|    Jan-19|    11-01-2019|\n",
            "+-------------+-----+----+----+----------+--------+---------------+------------+--------+---------+---------------+-----------------+-----------+-------------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfc.show()\n",
        "from pyspark.sql.functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOZhtVwu0ZED",
        "outputId": "d90b90b8-8938-4483-9fbb-caca9bd44f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+\n",
            "|               feeds|totalFeed|\n",
            "+--------------------+---------+\n",
            "|[{0, 0, 2020-01-0...|      125|\n",
            "+--------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rawDF = spark.read.json(\"data.json\", multiLine = \"true\")\n",
        "rawDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "K16ssJgA4_bW",
        "outputId": "0be5ea7c-0ee6-4ffb-be90-62b97c81a329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+-----+------+\n",
            "|Category| ID|Truth| Value|\n",
            "+--------+---+-----+------+\n",
            "|       A|  1| true|121.44|\n",
            "|       B|  5|false|300.01|\n",
            "|       G|  6|false|308.01|\n",
            "|       B|  7|false| 30.01|\n",
            "|       A|  8|false|306.01|\n",
            "|       B|  9| true|300.01|\n",
            "|       D| 10|false|300.01|\n",
            "|       B|  2|false|300.01|\n",
            "|       C|  3| null| 10.99|\n",
            "|       E|  4| true| 33.87|\n",
            "|       F|  4| true| 93.87|\n",
            "+--------+---+-----+------+\n",
            "\n"
          ]
        },
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bd19da1d64e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# when() otherwise()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruth\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruth\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"false\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New_Truth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'CASE WHEN (Truth = CAST('true' AS BOOLEAN)) THEN 'T' WHEN (Truth = CAST('false' AS BOOLEAN)) THEN 'F' WHEN (Truth IS NULL) THEN ' ' ELSE Truth END' due to data type mismatch: THEN and ELSE expressions should all be same type or coercible to a common type, got CASE WHEN ... THEN string WHEN ... THEN string WHEN ... THEN string ELSE boolean END;\n'Project [Category#0, ID#1L, Truth#2, Value#3, CASE WHEN (Truth#2 = cast(true as boolean)) THEN T WHEN (Truth#2 = cast(false as boolean)) THEN F WHEN isnull(Truth#2) THEN   ELSE Truth#2 END AS New_Truth#419]\n+- LogicalRDD [Category#0, ID#1L, Truth#2, Value#3], false\n"
          ]
        }
      ],
      "source": [
        "## Pyspark in-built functions\n",
        "# when() otherwise()\n",
        "df.show()\n",
        "df.select(col(\"*\"),when(df.Truth==\"true\", \"T\").when(df.Truth==\"false\", \"F\").when(df.Truth.isNull(), \" \").otherwise(df.Truth).alias(\"New_Truth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "li_OVjYK76bx",
        "outputId": "27a6c02b-a79c-47d7-abd0-50d651126b86"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-39db3d14c41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df3 = df.withColumn(\"new_truth\", expr(\"CASE WHEN Truth = 'true' THEN 'T' \" + \n\u001b[0;32m----> 2\u001b[0;31m                \u001b[0;34m\"WHEN Truth = 'false' THEN 'F' WHEN Truth IS NULL THEN ''\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                \"ELSE Truth END\"))\n\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'CASE WHEN (Truth = CAST('true' AS BOOLEAN)) THEN 'T' WHEN (Truth = CAST('false' AS BOOLEAN)) THEN 'F' WHEN (Truth IS NULL) THEN '' ELSE Truth END' due to data type mismatch: THEN and ELSE expressions should all be same type or coercible to a common type, got CASE WHEN ... THEN string WHEN ... THEN string WHEN ... THEN string ELSE boolean END; line 1 pos 0;\n'Project [Category#0, ID#1L, Truth#2, Value#3, CASE WHEN (Truth#2 = cast(true as boolean)) THEN T WHEN (Truth#2 = cast(false as boolean)) THEN F WHEN isnull(Truth#2) THEN  ELSE Truth#2 END AS new_truth#421]\n+- LogicalRDD [Category#0, ID#1L, Truth#2, Value#3], false\n"
          ]
        }
      ],
      "source": [
        "df3 = df.withColumn(\"new_truth\", expr(\"CASE WHEN Truth = 'true' THEN 'T' \" +\n",
        "               \"WHEN Truth = 'false' THEN 'F' WHEN Truth IS NULL THEN ''\" +\n",
        "               \"ELSE Truth END\"))\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np8wddpD_9bv",
        "outputId": "d02e6c01-3a46-4264-8569-1af818155fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- dob_year: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+--------------------+--------+------+------+\n",
            "|                name|dob_year|gender|salary|\n",
            "+--------------------+--------+------+------+\n",
            "|     James, A, Smith|    2018|     M|  3000|\n",
            "|Michael, Rose, Jones|    2010|     M|  4000|\n",
            "|   Robert,K,Williams|    2010|     M|  4000|\n",
            "|    Maria,Anne,Jones|    2005|     F|  4000|\n",
            "|      Jen,Mary,Brown|    2010|      |    -1|\n",
            "+--------------------+--------+------+------+\n",
            "\n",
            "+--------------------+\n",
            "|           Namearray|\n",
            "+--------------------+\n",
            "| [James,  A,  Smith]|\n",
            "|[Michael,  Rose, ...|\n",
            "|[Robert, K, Willi...|\n",
            "|[Maria, Anne, Jones]|\n",
            "|  [Jen, Mary, Brown]|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# split()\n",
        "from pyspark.sql.functions import split, col\n",
        "data1 = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
        "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
        "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
        "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
        "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
        "            ]\n",
        "\n",
        "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
        "dfsp=spark.createDataFrame(data1,columns)\n",
        "dfsp.printSchema()\n",
        "dfsp.show()\n",
        "dfsp_=dfsp.select(split(col(\"name\"), \",\").alias(\"Namearray\"))\n",
        "dfsp_.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdIg031gGgjD",
        "outputId": "0a5914c3-526d-45f8-e559-174981b358f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+------------+\n",
            "|name            |languagesAtSchool |currentState|\n",
            "+----------------+------------------+------------+\n",
            "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
            "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
            "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
            "+----------------+------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## concat_ws()\n",
        "from pyspark.sql.functions import col, concat_ws\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpS1UbPkGrJM",
        "outputId": "f6116388-ba9e-4ba5-9e57-c4b9f1e333d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|       Overlayed|\n",
            "+----------------+\n",
            "| JJava,Scala,C++|\n",
            "| MSpark,Java,C++|\n",
            "|RCSharp,VBlliams|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"languagesAtSchool\", concat_ws(\",\",col(\"languagesAtSchool\")))\n",
        "dfcon.select(overlay('name',\"languagesAtSchool\", 2).alias(\"Overlayed\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYqmZqcsJiol",
        "outputId": "3c50e817-052b-4813-ecef-34ad43f834b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------+-----+\n",
            "| id|           address|state|\n",
            "+---+------------------+-----+\n",
            "|  1|  14851 Jeffrey Rd|   DE|\n",
            "|  2|43421 Margarita St|   NY|\n",
            "|  3|  13111 Siemon Ave|   CA|\n",
            "+---+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Map\n",
        "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
        "    (2,\"43421 Margarita St\",\"NY\"),\n",
        "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
        "dfmap =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
        "dfmap.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pqcFtUKUYbyU"
      },
      "outputs": [],
      "source": [
        "data2 = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
        "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3lrEsLvJrm5",
        "outputId": "f863c234-1a09-4a48-9be4-78167fd7a423"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-43aa7327df3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m df2=df.rdd.map(lambda x: \n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstateDic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     ).toDF([\"id\",\"address\",\"state\"])\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \"\"\"\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 37) (df251d87dfac executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-43aa7327df3f>\", line 5, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'id' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-38-43aa7327df3f>\", line 5, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: id\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n",
        "rdd=spark.sparkContext.parallelize(address)\n",
        "\n",
        "df2=df.rdd.map(lambda x:\n",
        "    (x.id,x.address,stateDic[x.state])\n",
        "    ).toDF([\"id\",\"address\",\"state\"])\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlI8j1JNS1lE"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvrdgfuFtTfg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4E8MPnU/hytgdgxFZYOuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}